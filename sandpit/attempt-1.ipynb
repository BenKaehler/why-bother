{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd1779bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23a4c10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10fddee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-30 06:57:04.760653: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2021-08-30 06:57:04.780824: E tensorflow/stream_executor/cuda/cuda_driver.cc:313] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2021-08-30 06:57:04.780848: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: winter\n",
      "2021-08-30 06:57:04.780853: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: winter\n",
      "2021-08-30 06:57:04.780933: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.57.2\n",
      "2021-08-30 06:57:04.780957: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.57.2\n",
      "2021-08-30 06:57:04.780961: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.57.2\n"
     ]
    }
   ],
   "source": [
    "# thanks https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13923ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pa = np.array([[0.71893144, 0.14144765, 0.10698649, 0.03263443],\n",
    "       [0.06325843, 0.77423272, 0.13928361, 0.02322523],\n",
    "       [0.02322523, 0.13928361, 0.77423272, 0.06325843],\n",
    "       [0.03263443, 0.10698649, 0.14144765, 0.71893144]])\n",
    "\n",
    "Pb = np.array([[0.95154866, 0.01674797, 0.0234783 , 0.00822507],\n",
    "       [0.03863562, 0.84803839, 0.04732459, 0.0660014 ],\n",
    "       [0.0660014 , 0.04732459, 0.84803839, 0.03863562],\n",
    "       [0.00822507, 0.0234783 , 0.01674797, 0.95154866]])\n",
    "\n",
    "Pc = np.array([[0.88137945, 0.04613594, 0.02686734, 0.04561727],\n",
    "       [0.05009116, 0.9236776 , 0.0194023 , 0.00682894],\n",
    "       [0.00682894, 0.0194023 , 0.9236776 , 0.05009116],\n",
    "       [0.04561727, 0.02686734, 0.04613594, 0.88137945]])\n",
    "Pm = np.array([[0.81203158, 0.04399917, 0.07848442, 0.06548484],\n",
    "       [0.08284662, 0.87115553, 0.02024715, 0.0257507 ],\n",
    "       [0.0257507 , 0.02024715, 0.87115553, 0.08284662],\n",
    "       [0.06548484, 0.07848442, 0.04399917, 0.81203158]])\n",
    "\n",
    "pi = np.array([0.14977394, 0.26317362, 0.39165429, 0.19539816])\n",
    "\n",
    "pi_m = pi @ Pm\n",
    "\n",
    "J = np.array([[[0.07508613297288518, 0.00466350788803318, 0.0033432901131033796, 0.004042219322757277], [0.002186937387234338, 0.01570096614179908, 0.0011254388766303509, 0.0005015574893929194], [0.002007283482847851, 0.0012535337368050612, 0.013395860029779497, 0.0010220788989917808], [0.0013011086711092986, 0.0016042909684947522, 0.0012404869820179004, 0.011323417297654589]], [[0.03129698648252512, 0.00815953485072002, 0.004394876926278819, 0.0020308813605868174], [0.008288184321098279, 0.14201837079233715, 0.005398759767993272, 0.0017880581744532536], [0.0015277493099248596, 0.00886482820182737, 0.04269081732021271, 0.0028169181161841477], [0.0021015052010260205, 0.011819352252411985, 0.0034103593745217937, 0.02379160092199082]], [[0.02174460265532043, 0.002941403304710777, 0.017006110729978843, 0.0023681733984058486], [0.0022577498981608663, 0.03237071808690972, 0.012426756488990255, 0.0019160961759307586], [0.0022144999652547693, 0.006239396030284971, 0.20953281339937846, 0.012130269098422072], [0.002543170970124578, 0.003973434481032667, 0.011775618913078878, 0.04210892174683528]], [[0.012093125924573445, 0.0012993605630256348, 0.0021464967072447926, 0.0015645368675293004], [0.0010702747292501203, 0.013479197587703494, 0.0016493822206027895, 0.0025896804160072057], [0.0005894417935184289, 0.00127918145334602, 0.022260289874459077, 0.0029430040436876307], [0.005229012301832341, 0.0040480932067737535, 0.006152859243062461, 0.09785943409093556]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65573e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "J_mb = pi_m[:, np.newaxis] * Pb\n",
    "J_bc = J_mb.T @ Pc\n",
    "J_ab = Pa.T @ np.diag(pi) @ Pm @ Pb\n",
    "J_ca = (Pa.T @ np.diag(pi) @ Pm @ Pc).T\n",
    "\n",
    "assert np.allclose(J_bc, J.sum(axis=0))  # checking I have constructed it correctly\n",
    "assert np.allclose(J_ab, J.sum(axis=2))\n",
    "assert np.allclose(J_ca, J.sum(axis=1).T)\n",
    "\n",
    "J_einsum = np.einsum('i,ij,ik,ku,kv', pi, Pa, Pm, Pb, Pc)\n",
    "assert np.allclose(J_einsum, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "079f283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thanks https://github.com/mlgxmez/thelongrun_notebooks/blob/master/MLE_tutorial.ipynb\n",
    "\n",
    "@tf.function()\n",
    "def transform(params):\n",
    "    pi = tfb.SoftmaxCentered()(params[0])\n",
    "    Pa = tfb.SoftmaxCentered()(params[1:5])\n",
    "    Pm = tfb.SoftmaxCentered()(params[5:9])\n",
    "    Pb = tfb.SoftmaxCentered()(params[9:13])\n",
    "    Pc = tfb.SoftmaxCentered()(params[13:17])\n",
    "    return pi, Pa, Pm, Pb, Pc\n",
    "    \n",
    "@tf.function()\n",
    "def loss(params, data):\n",
    "    pi, Pa, Pm, Pb, Pc = transform(params)\n",
    "    J = tf.einsum('i,ij,ik,ku,kv', pi, Pa, Pm, Pb, Pc)\n",
    "    loss = tf.reduce_sum(tf.keras.losses.KLDivergence()(J, data))\n",
    "    return loss\n",
    "\n",
    "def mle_run(data, loss, parameters, optimizer, steps=1000, verbose=False):\n",
    "    update_list = []\n",
    "    \n",
    "    for i in range(steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = loss(parameters, data)\n",
    "        gradients = tape.gradient(loss_value, parameters)\n",
    "        optimizer.apply_gradients([(gradients, parameters)])\n",
    "    \n",
    "        if i % 10 == 0:\n",
    "            update_list.append((\n",
    "                optimizer.iterations.numpy(),loss_value.numpy(), \n",
    "                parameters.numpy()))\n",
    "            iter_info = f\"Step: {optimizer.iterations.numpy()}, initial loss: {loss_value.numpy()}\"\n",
    "            if verbose:\n",
    "                print(iter_info)\n",
    "    \n",
    "    return update_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ffc4eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1, initial loss: 0.06791029125452042\n",
      "Step: 11, initial loss: 0.06488090008497238\n",
      "Step: 21, initial loss: 0.06276635825634003\n",
      "Step: 31, initial loss: 0.061023350805044174\n",
      "Step: 41, initial loss: 0.0595443993806839\n",
      "Step: 51, initial loss: 0.05813857167959213\n",
      "Step: 61, initial loss: 0.05645224079489708\n",
      "Step: 71, initial loss: 0.05405370891094208\n",
      "Step: 81, initial loss: 0.05076826363801956\n",
      "Step: 91, initial loss: 0.046919479966163635\n",
      "Step: 101, initial loss: 0.043074071407318115\n",
      "Step: 111, initial loss: 0.0395585298538208\n",
      "Step: 121, initial loss: 0.03618793934583664\n",
      "Step: 131, initial loss: 0.03273250162601471\n",
      "Step: 141, initial loss: 0.029466306790709496\n",
      "Step: 151, initial loss: 0.026662997901439667\n",
      "Step: 161, initial loss: 0.024276018142700195\n",
      "Step: 171, initial loss: 0.022198786959052086\n",
      "Step: 181, initial loss: 0.0204582829028368\n",
      "Step: 191, initial loss: 0.019036371260881424\n",
      "Step: 201, initial loss: 0.017928624525666237\n",
      "Step: 211, initial loss: 0.017046235501766205\n",
      "Step: 221, initial loss: 0.0162275992333889\n",
      "Step: 231, initial loss: 0.015368983149528503\n",
      "Step: 241, initial loss: 0.01451125554740429\n",
      "Step: 251, initial loss: 0.013789225369691849\n",
      "Step: 261, initial loss: 0.013277750462293625\n",
      "Step: 271, initial loss: 0.0129505954682827\n",
      "Step: 281, initial loss: 0.012740647420287132\n",
      "Step: 291, initial loss: 0.012597457505762577\n",
      "Step: 301, initial loss: 0.012493560090661049\n",
      "Step: 311, initial loss: 0.012414171360433102\n",
      "Step: 321, initial loss: 0.012350018136203289\n",
      "Step: 331, initial loss: 0.012294710613787174\n",
      "Step: 341, initial loss: 0.012243900448083878\n",
      "Step: 351, initial loss: 0.01219464186578989\n",
      "Step: 361, initial loss: 0.01214485615491867\n",
      "Step: 371, initial loss: 0.012092729099094868\n",
      "Step: 381, initial loss: 0.012036518193781376\n",
      "Step: 391, initial loss: 0.011974316090345383\n",
      "Step: 401, initial loss: 0.011903807520866394\n",
      "Step: 411, initial loss: 0.011822076514363289\n",
      "Step: 421, initial loss: 0.011725475080311298\n",
      "Step: 431, initial loss: 0.01160933542996645\n",
      "Step: 441, initial loss: 0.011467758566141129\n",
      "Step: 451, initial loss: 0.011293509043753147\n",
      "Step: 461, initial loss: 0.011078059673309326\n",
      "Step: 471, initial loss: 0.010812034830451012\n",
      "Step: 481, initial loss: 0.010486403480172157\n",
      "Step: 491, initial loss: 0.01009440328925848\n",
      "Step: 501, initial loss: 0.009634695947170258\n",
      "Step: 511, initial loss: 0.009114977903664112\n",
      "Step: 521, initial loss: 0.008554928004741669\n",
      "Step: 531, initial loss: 0.007985461503267288\n",
      "Step: 541, initial loss: 0.007441713009029627\n",
      "Step: 551, initial loss: 0.006951909512281418\n",
      "Step: 561, initial loss: 0.006529786624014378\n",
      "Step: 571, initial loss: 0.00617531081661582\n",
      "Step: 581, initial loss: 0.005879295989871025\n",
      "Step: 591, initial loss: 0.0056274812668561935\n",
      "Step: 601, initial loss: 0.005401637405157089\n",
      "Step: 611, initial loss: 0.005177902057766914\n",
      "Step: 621, initial loss: 0.004924186039716005\n",
      "Step: 631, initial loss: 0.004602786153554916\n",
      "Step: 641, initial loss: 0.004189412109553814\n",
      "Step: 651, initial loss: 0.003687820630148053\n",
      "Step: 661, initial loss: 0.0031214081682264805\n",
      "Step: 671, initial loss: 0.002551203826442361\n",
      "Step: 681, initial loss: 0.002033249707892537\n",
      "Step: 691, initial loss: 0.001598946750164032\n",
      "Step: 701, initial loss: 0.0012543920893222094\n",
      "Step: 711, initial loss: 0.0009900532895699143\n",
      "Step: 721, initial loss: 0.0007907926337793469\n",
      "Step: 731, initial loss: 0.0006412995280697942\n",
      "Step: 741, initial loss: 0.0005284184007905424\n",
      "Step: 751, initial loss: 0.0004419650649651885\n",
      "Step: 761, initial loss: 0.0003746894944924861\n",
      "Step: 771, initial loss: 0.0003216000332031399\n",
      "Step: 781, initial loss: 0.0002792638260871172\n",
      "Step: 791, initial loss: 0.00024522983585484326\n",
      "Step: 801, initial loss: 0.00021767965517938137\n",
      "Step: 811, initial loss: 0.0001952228049049154\n",
      "Step: 821, initial loss: 0.00017676923016551882\n",
      "Step: 831, initial loss: 0.00016149069415405393\n",
      "Step: 841, initial loss: 0.00014869040751364082\n",
      "Step: 851, initial loss: 0.0001378545130137354\n",
      "Step: 861, initial loss: 0.00012858335685450584\n",
      "Step: 871, initial loss: 0.00012054009130224586\n",
      "Step: 881, initial loss: 0.00011349724809406325\n",
      "Step: 891, initial loss: 0.0001072629529517144\n",
      "Step: 901, initial loss: 0.00010168013977818191\n",
      "Step: 911, initial loss: 9.66505249380134e-05\n",
      "Step: 921, initial loss: 9.207488619722426e-05\n",
      "Step: 931, initial loss: 8.788310515228659e-05\n",
      "Step: 941, initial loss: 8.401992090512067e-05\n",
      "Step: 951, initial loss: 8.04425508249551e-05\n",
      "Step: 961, initial loss: 7.71116028772667e-05\n",
      "Step: 971, initial loss: 7.399097376037389e-05\n",
      "Step: 981, initial loss: 7.106983684934676e-05\n",
      "Step: 991, initial loss: 6.831296195741743e-05\n",
      "Step: 1001, initial loss: 6.570751429535449e-05\n",
      "Step: 1011, initial loss: 6.326200673356652e-05\n",
      "Step: 1021, initial loss: 6.093350384617224e-05\n",
      "Step: 1031, initial loss: 5.873385089216754e-05\n",
      "Step: 1041, initial loss: 5.664450873155147e-05\n",
      "Step: 1051, initial loss: 5.465716458274983e-05\n",
      "Step: 1061, initial loss: 5.27698575751856e-05\n",
      "Step: 1071, initial loss: 5.098705878481269e-05\n",
      "Step: 1081, initial loss: 4.928294220007956e-05\n",
      "Step: 1091, initial loss: 4.7660170821473e-05\n",
      "Step: 1101, initial loss: 4.612906195688993e-05\n",
      "Step: 1111, initial loss: 4.466338941711001e-05\n",
      "Step: 1121, initial loss: 4.3263236875645816e-05\n",
      "Step: 1131, initial loss: 4.193054337520152e-05\n",
      "Step: 1141, initial loss: 4.066176188644022e-05\n",
      "Step: 1151, initial loss: 3.94647940993309e-05\n",
      "Step: 1161, initial loss: 3.8305894122458994e-05\n",
      "Step: 1171, initial loss: 3.71967616956681e-05\n",
      "Step: 1181, initial loss: 3.615249806898646e-05\n",
      "Step: 1191, initial loss: 3.513235424179584e-05\n",
      "Step: 1201, initial loss: 3.4168951970059425e-05\n",
      "Step: 1211, initial loss: 3.324212957522832e-05\n",
      "Step: 1221, initial loss: 3.23559288517572e-05\n",
      "Step: 1231, initial loss: 3.1507835956290364e-05\n",
      "Step: 1241, initial loss: 3.067308716708794e-05\n",
      "Step: 1251, initial loss: 2.988237247336656e-05\n",
      "Step: 1261, initial loss: 2.912565651058685e-05\n",
      "Step: 1271, initial loss: 2.839604530890938e-05\n",
      "Step: 1281, initial loss: 2.7685997338267043e-05\n",
      "Step: 1291, initial loss: 2.7000423870049417e-05\n",
      "Step: 1301, initial loss: 2.633811345731374e-05\n",
      "Step: 1311, initial loss: 2.570468495832756e-05\n",
      "Step: 1321, initial loss: 2.5074990844586864e-05\n",
      "Step: 1331, initial loss: 2.4480128558934666e-05\n",
      "Step: 1341, initial loss: 2.3894190235296264e-05\n",
      "Step: 1351, initial loss: 2.3343372959061526e-05\n",
      "Step: 1361, initial loss: 2.2791502487962134e-05\n",
      "Step: 1371, initial loss: 2.226963624707423e-05\n",
      "Step: 1381, initial loss: 2.1740277588833123e-05\n",
      "Step: 1391, initial loss: 2.125179162248969e-05\n",
      "Step: 1401, initial loss: 2.0765834051417187e-05\n",
      "Step: 1411, initial loss: 2.028540984611027e-05\n",
      "Step: 1421, initial loss: 1.98244524654001e-05\n",
      "Step: 1431, initial loss: 1.9380599042051472e-05\n",
      "Step: 1441, initial loss: 1.8948421711684205e-05\n",
      "Step: 1451, initial loss: 1.8520686353440396e-05\n",
      "Step: 1461, initial loss: 1.8112663383362815e-05\n",
      "Step: 1471, initial loss: 1.771337520040106e-05\n",
      "Step: 1481, initial loss: 1.73151092894841e-05\n",
      "Step: 1491, initial loss: 1.694157617748715e-05\n",
      "Step: 1501, initial loss: 1.655594678595662e-05\n",
      "Step: 1511, initial loss: 1.6201425751205534e-05\n",
      "Step: 1521, initial loss: 1.5849596820771694e-05\n",
      "Step: 1531, initial loss: 1.549497937958222e-05\n",
      "Step: 1541, initial loss: 1.5162064300966449e-05\n",
      "Step: 1551, initial loss: 1.483599771745503e-05\n",
      "Step: 1561, initial loss: 1.4509477296087425e-05\n",
      "Step: 1571, initial loss: 1.41936161526246e-05\n",
      "Step: 1581, initial loss: 1.3888180546928197e-05\n",
      "Step: 1591, initial loss: 1.3577383469964843e-05\n",
      "Step: 1601, initial loss: 1.3291770301293582e-05\n",
      "Step: 1611, initial loss: 1.30043508761446e-05\n",
      "Step: 1621, initial loss: 1.271652308787452e-05\n",
      "Step: 1631, initial loss: 1.2441435501386877e-05\n",
      "Step: 1641, initial loss: 1.216990585817257e-05\n",
      "Step: 1651, initial loss: 1.1915362847503275e-05\n",
      "Step: 1661, initial loss: 1.1659761185001116e-05\n",
      "Step: 1671, initial loss: 1.1403988537495025e-05\n",
      "Step: 1681, initial loss: 1.1162561349919997e-05\n",
      "Step: 1691, initial loss: 1.0925825336016715e-05\n",
      "Step: 1701, initial loss: 1.0689733244362287e-05\n",
      "Step: 1711, initial loss: 1.0445662155689206e-05\n",
      "Step: 1721, initial loss: 1.023711320158327e-05\n",
      "Step: 1731, initial loss: 1.0015283805842046e-05\n",
      "Step: 1741, initial loss: 9.791898264666088e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1751, initial loss: 9.58787313720677e-06\n",
      "Step: 1761, initial loss: 9.37754703045357e-06\n",
      "Step: 1771, initial loss: 9.180257620755583e-06\n",
      "Step: 1781, initial loss: 8.975812306744047e-06\n",
      "Step: 1791, initial loss: 8.77881575434003e-06\n",
      "Step: 1801, initial loss: 8.600445653428324e-06\n",
      "Step: 1811, initial loss: 8.412662282353267e-06\n",
      "Step: 1821, initial loss: 8.224800694733858e-06\n",
      "Step: 1831, initial loss: 8.056363185460214e-06\n",
      "Step: 1841, initial loss: 7.883759280957747e-06\n",
      "Step: 1851, initial loss: 7.708665179961827e-06\n",
      "Step: 1861, initial loss: 7.545745120296488e-06\n",
      "Step: 1871, initial loss: 7.384719083347591e-06\n",
      "Step: 1881, initial loss: 7.232208190544043e-06\n",
      "Step: 1891, initial loss: 7.065909812808968e-06\n",
      "Step: 1901, initial loss: 6.9183643063297495e-06\n",
      "Step: 1911, initial loss: 6.77016669214936e-06\n",
      "Step: 1921, initial loss: 6.620727162953699e-06\n",
      "Step: 1931, initial loss: 6.479394414782291e-06\n",
      "Step: 1941, initial loss: 6.333907094813185e-06\n",
      "Step: 1951, initial loss: 6.198831215442624e-06\n",
      "Step: 1961, initial loss: 6.07310539635364e-06\n",
      "Step: 1971, initial loss: 5.9318026615073904e-06\n",
      "Step: 1981, initial loss: 5.813271400256781e-06\n",
      "Step: 1991, initial loss: 5.682832579623209e-06\n",
      "Step: 2001, initial loss: 5.550511104956968e-06\n",
      "Step: 2011, initial loss: 5.430495548353065e-06\n",
      "Step: 2021, initial loss: 5.323753157426836e-06\n",
      "Step: 2031, initial loss: 5.206586138228886e-06\n",
      "Step: 2041, initial loss: 5.086821147415321e-06\n",
      "Step: 2051, initial loss: 4.968750999978511e-06\n",
      "Step: 2061, initial loss: 4.866284143645316e-06\n",
      "Step: 2071, initial loss: 4.765548965224298e-06\n",
      "Step: 2081, initial loss: 4.658320904127322e-06\n",
      "Step: 2091, initial loss: 4.566246389003936e-06\n",
      "Step: 2101, initial loss: 4.455288944882341e-06\n",
      "Step: 2111, initial loss: 4.35659967479296e-06\n",
      "Step: 2121, initial loss: 4.266764335625339e-06\n",
      "Step: 2131, initial loss: 4.167478437011596e-06\n",
      "Step: 2141, initial loss: 4.0728459680394735e-06\n",
      "Step: 2151, initial loss: 3.985447619925253e-06\n",
      "Step: 2161, initial loss: 3.902484877471579e-06\n",
      "Step: 2171, initial loss: 3.820437996182591e-06\n",
      "Step: 2181, initial loss: 3.726630666278652e-06\n",
      "Step: 2191, initial loss: 3.6460899082157994e-06\n",
      "Step: 2201, initial loss: 3.573840785975335e-06\n",
      "Step: 2211, initial loss: 3.4838490137190092e-06\n",
      "Step: 2221, initial loss: 3.4195900298072957e-06\n",
      "Step: 2231, initial loss: 3.3411706681363285e-06\n",
      "Step: 2241, initial loss: 3.2566092613706132e-06\n",
      "Step: 2251, initial loss: 3.181824922648957e-06\n",
      "Step: 2261, initial loss: 3.1112249416764826e-06\n",
      "Step: 2271, initial loss: 3.0467999749816954e-06\n",
      "Step: 2281, initial loss: 2.9767870728392154e-06\n",
      "Step: 2291, initial loss: 2.923652345998562e-06\n",
      "Step: 2301, initial loss: 2.8575511805684073e-06\n",
      "Step: 2311, initial loss: 2.791262431856012e-06\n",
      "Step: 2321, initial loss: 2.7254081942373887e-06\n",
      "Step: 2331, initial loss: 2.669153900569654e-06\n",
      "Step: 2341, initial loss: 2.599888830445707e-06\n",
      "Step: 2351, initial loss: 2.5426020329177845e-06\n",
      "Step: 2361, initial loss: 2.4848839075275464e-06\n",
      "Step: 2371, initial loss: 2.426949095024611e-06\n",
      "Step: 2381, initial loss: 2.3762438559060683e-06\n",
      "Step: 2391, initial loss: 2.325222112631309e-06\n",
      "Step: 2401, initial loss: 2.2562826416105963e-06\n",
      "Step: 2411, initial loss: 2.215970198449213e-06\n",
      "Step: 2421, initial loss: 2.167791762985871e-06\n",
      "Step: 2431, initial loss: 2.1090304471726995e-06\n",
      "Step: 2441, initial loss: 2.0645895801862935e-06\n",
      "Step: 2451, initial loss: 2.016802682192065e-06\n",
      "Step: 2461, initial loss: 1.974041879293509e-06\n",
      "Step: 2471, initial loss: 1.9290960153739434e-06\n",
      "Step: 2481, initial loss: 1.8879743493016576e-06\n",
      "Step: 2491, initial loss: 1.838101070461562e-06\n",
      "Step: 2501, initial loss: 1.7945089894055855e-06\n",
      "Step: 2511, initial loss: 1.7555785234435461e-06\n",
      "Step: 2521, initial loss: 1.7085621948353946e-06\n",
      "Step: 2531, initial loss: 1.6708518160157837e-06\n",
      "Step: 2541, initial loss: 1.6379256067011738e-06\n",
      "Step: 2551, initial loss: 1.5962255019985605e-06\n",
      "Step: 2561, initial loss: 1.5703499229857698e-06\n",
      "Step: 2571, initial loss: 1.5297957816073904e-06\n",
      "Step: 2581, initial loss: 1.4780750916543184e-06\n",
      "Step: 2591, initial loss: 1.443726659999811e-06\n",
      "Step: 2601, initial loss: 1.421663910150528e-06\n",
      "Step: 2611, initial loss: 1.3830001535097836e-06\n",
      "Step: 2621, initial loss: 1.3529445368476445e-06\n",
      "Step: 2631, initial loss: 1.3137557743903017e-06\n",
      "Step: 2641, initial loss: 1.2804739526472986e-06\n",
      "Step: 2651, initial loss: 1.2468576642277185e-06\n",
      "Step: 2661, initial loss: 1.2302512004680466e-06\n",
      "Step: 2671, initial loss: 1.2026998774672393e-06\n",
      "Step: 2681, initial loss: 1.1661916232696967e-06\n",
      "Step: 2691, initial loss: 1.1323531907692086e-06\n",
      "Step: 2701, initial loss: 1.1097581591457129e-06\n",
      "Step: 2711, initial loss: 1.0924438811343862e-06\n",
      "Step: 2721, initial loss: 1.065969399860478e-06\n",
      "Step: 2731, initial loss: 1.0346075214329176e-06\n",
      "Step: 2741, initial loss: 1.0039670996775385e-06\n",
      "Step: 2751, initial loss: 9.753605354489991e-07\n",
      "Step: 2761, initial loss: 9.667908216215437e-07\n",
      "Step: 2771, initial loss: 9.370955922349822e-07\n",
      "Step: 2781, initial loss: 9.185625913232798e-07\n",
      "Step: 2791, initial loss: 8.924395160647691e-07\n",
      "Step: 2801, initial loss: 8.68905772222206e-07\n",
      "Step: 2811, initial loss: 8.481937356918934e-07\n",
      "Step: 2821, initial loss: 8.268589226645418e-07\n",
      "Step: 2831, initial loss: 8.109415148283006e-07\n",
      "Step: 2841, initial loss: 7.856868933231453e-07\n",
      "Step: 2851, initial loss: 7.729539674983243e-07\n",
      "Step: 2861, initial loss: 7.514579465350835e-07\n",
      "Step: 2871, initial loss: 7.197380682555377e-07\n",
      "Step: 2881, initial loss: 7.151950285333442e-07\n",
      "Step: 2891, initial loss: 6.889115411468083e-07\n",
      "Step: 2901, initial loss: 6.784101742596249e-07\n",
      "Step: 2911, initial loss: 6.517838073705207e-07\n",
      "Step: 2921, initial loss: 6.28724023954419e-07\n",
      "Step: 2931, initial loss: 6.312304776656674e-07\n",
      "Step: 2941, initial loss: 6.127544338596635e-07\n",
      "Step: 2951, initial loss: 5.90715558246302e-07\n",
      "Step: 2961, initial loss: 5.872317387911608e-07\n",
      "Step: 2971, initial loss: 5.554230710913544e-07\n",
      "Step: 2981, initial loss: 5.470850510391756e-07\n",
      "Step: 2991, initial loss: 5.337059292287449e-07\n"
     ]
    }
   ],
   "source": [
    "normal_initializer = tf.random_normal_initializer()\n",
    "params = tf.Variable(normal_initializer(shape=[17, 3], dtype=tf.float32), name='params')\n",
    "\n",
    "optimizer = tf.optimizers.Adam(0.01)\n",
    "update_list = mle_run(J, loss, params, optimizer, steps=3000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2dcc4fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14977394 0.26317362 0.39165429 0.19539816]\n",
      "[0.17649218 0.38592875 0.14713755 0.29044157]\n",
      "\n",
      "[[0.71893144 0.14144765 0.10698649 0.03263443]\n",
      " [0.06325843 0.77423272 0.13928361 0.02322523]\n",
      " [0.02322523 0.13928361 0.77423272 0.06325843]\n",
      " [0.03263443 0.10698649 0.14144765 0.71893144]]\n",
      "[[0.05230147 0.09148193 0.1839756  0.672241  ]\n",
      " [0.0419483  0.13268892 0.7509073  0.07445542]\n",
      " [0.64273643 0.14791214 0.13135116 0.07800025]\n",
      " [0.06818739 0.7274398  0.14446567 0.05990707]]\n",
      "\n",
      "[[0.81203158 0.04399917 0.07848442 0.06548484]\n",
      " [0.08284662 0.87115553 0.02024715 0.0257507 ]\n",
      " [0.0257507  0.02024715 0.87115553 0.08284662]\n",
      " [0.06548484 0.07848442 0.04399917 0.81203158]]\n",
      "[[0.01675984 0.01031547 0.01294123 0.95998347]\n",
      " [0.01417217 0.9158386  0.0149206  0.05506857]\n",
      " [0.92680544 0.01518238 0.02910416 0.02890802]\n",
      " [0.07432606 0.03245734 0.85027885 0.04293775]]\n",
      "\n",
      "[[0.95154866 0.01674797 0.0234783  0.00822507]\n",
      " [0.03863562 0.84803839 0.04732459 0.0660014 ]\n",
      " [0.0660014  0.04732459 0.84803839 0.03863562]\n",
      " [0.00822507 0.0234783  0.01674797 0.95154866]]\n",
      "[[0.95096093 0.01625519 0.02331331 0.00947065]\n",
      " [0.06599598 0.04743445 0.84791905 0.03865059]\n",
      " [0.03865016 0.84804845 0.04734506 0.06595635]\n",
      " [0.0082203  0.02290418 0.01676725 0.95210826]]\n",
      "\n",
      "[[0.88137945 0.04613594 0.02686734 0.04561727]\n",
      " [0.05009116 0.9236776  0.0194023  0.00682894]\n",
      " [0.00682894 0.0194023  0.9236776  0.05009116]\n",
      " [0.04561727 0.02686734 0.04613594 0.88137945]]\n",
      "[[0.88141525 0.04613197 0.02688022 0.04557255]\n",
      " [0.00686434 0.01939924 0.9237153  0.05002103]\n",
      " [0.05021285 0.92298555 0.01909447 0.00770719]\n",
      " [0.04500836 0.02685685 0.04613423 0.8820006 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p, phat in zip([pi, Pa, Pm, Pb, Pc], transform(params)):\n",
    "    print(p)\n",
    "    print(phat.numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c262d48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
