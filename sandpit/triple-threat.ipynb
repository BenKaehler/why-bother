{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95afc4bd",
   "metadata": {},
   "source": [
    "# `triple-threat` proof of concept\n",
    "Proof of concept for a triple-fitting, cherry-picking, tree-building algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71f1501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from cogent3 import load_aligned_seqs, PhyloNode\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a792d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "tf.executing_eagerly()  # need to check whether this is the default for tensorflow > 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccbca69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this stops tensorflow from snaffling all of the GPU\n",
    "# thanks https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4daac3",
   "metadata": {},
   "source": [
    "## Data import\n",
    "Reads an alignments and creates a list of 4 x 4 x 4 joint frequencies tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81060425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_triples(aln, nuc_order='ACGT', codon_position=None, verbose=False):\n",
    "    if codon_position:\n",
    "        aln = aln[codon_position - 1::3]\n",
    "    aln = aln.no_degenerates()\n",
    "    if verbose:\n",
    "        print(f'Got {len(aln)} positions')\n",
    "    assert len(aln) <= np.iinfo(np.int32).max\n",
    "    triples = []\n",
    "    nuc_map = {n:i for i, n in enumerate(nuc_order)}\n",
    "    for triple in combinations(range(aln.num_seqs), 3):\n",
    "        F = np.zeros([4, 4, 4], dtype=np.int32)\n",
    "        subaln = aln.get_sub_alignment(seqs=triple)\n",
    "        for a, b, c in subaln:\n",
    "            F[nuc_map[a], nuc_map[b], nuc_map[c]] += 1\n",
    "        triples.append([tuple(subaln.names), F])\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7937f2",
   "metadata": {},
   "source": [
    "## Triple fitting functions\n",
    "Collection of functions for concurrent fitting of many triples on CPUs and GPUs. Model is rooted, continuous-time, and strand-symmetric.\n",
    "\n",
    "Also some functions for using Akaike-ish weights to find the pair that is most probably a cherry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ddd765",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def transform_P_matrix(params):\n",
    "    params = tf.exp(params)\n",
    "    Q0 = tf.concat([[-tf.reduce_sum(params[0])], params[0]],\n",
    "                   axis=0)\n",
    "    Q1 = tf.concat([[params[1,0]], [-tf.reduce_sum(params[1])], params[1,1:]],\n",
    "                   axis=0)\n",
    "    Q = tf.concat([[Q0], [Q1], [Q1[::-1]], [Q0[::-1]]], axis=0)\n",
    "    return tf.linalg.expm(Q)\n",
    "\n",
    "@tf.function()\n",
    "def transform(params):\n",
    "    pi = tfb.SoftmaxCentered()(params[0])\n",
    "    Pa = transform_P_matrix(params[1:3])\n",
    "    Pm = transform_P_matrix(params[3:5])\n",
    "    Pb = transform_P_matrix(params[5:7])\n",
    "    Pc = transform_P_matrix(params[7:9])\n",
    "    return pi, Pa, Pm, Pb, Pc\n",
    "    \n",
    "@tf.function()\n",
    "def _loss(params_data):\n",
    "    params, data = params_data\n",
    "    pi, Pa, Pm, Pb, Pc = transform(params)\n",
    "    J = tf.einsum('i,ij,ik,ku,kv', pi, Pa, Pm, Pb, Pc)\n",
    "    loss = tf.reduce_sum(tf.keras.losses.KLDivergence()(J, data))\n",
    "    return loss\n",
    "    \n",
    "@tf.function()\n",
    "def loss(params, data):\n",
    "    # could do better managing the variance in the shared matrix case here\n",
    "    return tf.reduce_sum(tf.vectorized_map(_loss, (params, data)))\n",
    "\n",
    "@tf.function()\n",
    "def training_step(parameters, data, optimizer, unscrambler):\n",
    "    with tf.GradientTape() as tape:\n",
    "        unscrambled = _unscramble(parameters, unscrambler)\n",
    "        loss_value = loss(unscrambled, data)\n",
    "    gradients = tape.gradient(loss_value, parameters)\n",
    "    return loss_value, gradients\n",
    "\n",
    "# thanks https://github.com/mlgxmez/thelongrun_notebooks/blob/master/MLE_tutorial.ipynb\n",
    "def mle_fit(data, loss, parameters, optimizer, unscrambler, steps=500, verbose=False):\n",
    "    for i in range(steps):\n",
    "        loss_value, gradients = training_step(parameters, data, optimizer, unscrambler)\n",
    "        optimizer.apply_gradients([(gradients, parameters)])\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            if verbose:\n",
    "                iter_info = f\"Step: {optimizer.iterations.numpy()}, initial loss: {loss_value.numpy()}\"\n",
    "                print(iter_info)\n",
    "\n",
    "def check_cherry(ls, N):\n",
    "    ls = N*ls\n",
    "    delta = ls - ls.min()\n",
    "    weights = np.exp(-delta)\n",
    "    return weights/weights.sum()\n",
    "\n",
    "@tf.function()\n",
    "def _unscramble(parameters, unscramble):\n",
    "    unscrambled = []\n",
    "    for t1 in unscramble:\n",
    "        unscrambled.append(tf.stack([parameters[i] for i in t1]))\n",
    "    return tf.stack(unscrambled)\n",
    "\n",
    "def fit_triples(triples, learning_rate=0.01, cherries_share_matrices=True, steps=3000, verbose=False):\n",
    "    K = 0\n",
    "    cherry_loc = {}\n",
    "    unscrambler = []\n",
    "    data = []\n",
    "    for names, F in triples:\n",
    "        J = (F/F.sum()).astype(np.float32)\n",
    "        for ix in [[0, 1, 2], [1, 2, 0], [2, 0, 1]]:\n",
    "            t1 = list(range(K, K+5)) # for pi and two Ps\n",
    "            K += 5\n",
    "            cherry = [names[ix[1]], names[ix[2]]]\n",
    "            frozen_cherry = frozenset(cherry)\n",
    "            if not cherries_share_matrices or frozen_cherry not in cherry_loc:\n",
    "                new_loc = {cherry[0]: [K,K+1], cherry[1]: [K+2,K+3]}\n",
    "                K += 4\n",
    "                cherry_loc[frozen_cherry] = new_loc\n",
    "            t1.extend(cherry_loc[frozen_cherry][cherry[0]]) # for the first cherry\n",
    "            t1.extend(cherry_loc[frozen_cherry][cherry[1]]) # for the second cherry\n",
    "            unscrambler.append(t1)\n",
    "        \n",
    "            data.append(J.transpose(ix))\n",
    "\n",
    "    normal_initializer = tf.random_normal_initializer()\n",
    "    parameters = tf.Variable(normal_initializer(shape=[K, 3], dtype=tf.float32), name='params')\n",
    "    data = tf.stack(data)\n",
    "\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "    mle_fit(data, loss, parameters, optimizer, unscrambler, steps=steps, verbose=verbose)\n",
    "    \n",
    "    parameters = _unscramble(parameters, unscrambler)\n",
    "    losses = tf.vectorized_map(_loss, (parameters, data)).numpy()\n",
    "    losses = [losses[i:i+3] for i in range(0, len(losses), 3)]\n",
    "    fits = [[p.numpy() for p in transform(params)] for params in parameters]\n",
    "    fits = [fits[i:i+3] for i in range(0, len(fits), 3)]\n",
    "    return losses, fits\n",
    "\n",
    "def pick_cherry(root_probs):\n",
    "    cherry_llik = Counter()\n",
    "    for probs in root_probs:\n",
    "        names = frozenset(probs.keys())\n",
    "        for name in names:\n",
    "            cherry_llik[names - {name}] += np.log(probs[name])\n",
    "    return cherry_llik.most_common()\n",
    "\n",
    "def get_cherries(triples, losses, verbose=False):\n",
    "    root_probs = []\n",
    "    for losses, (names, F) in zip(losses, triples):\n",
    "        probs = check_cherry(losses, F.sum())\n",
    "        root_probs.append(dict(zip(names, probs)))\n",
    "    \n",
    "    cherry_llik = pick_cherry(root_probs)\n",
    "    if verbose:\n",
    "        for (n1, n2), ll in cherry_llik:\n",
    "            print(f'{n1}, {n2}: {ll}')\n",
    "    \n",
    "    return list(cherry_llik[0][0])\n",
    "\n",
    "def get_Ps(cherry_names, triples, fits):\n",
    "    Ps = {}\n",
    "    ixes = [[0, 1, 2], [1, 2, 0], [2, 0, 1]]\n",
    "    for (names, _), triple_fit in zip(triples, fits):\n",
    "        if set(cherry_names) < set(names):\n",
    "            for name, ix, fit in zip(names, ixes, triple_fit):\n",
    "                if name not in cherry_names:\n",
    "                    if names[ix[1]] == cherry_names[0]:\n",
    "                        return fit[-2], fit[-1]\n",
    "                    return fit[-1], fit[-2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284125ea",
   "metadata": {},
   "source": [
    "## Merge-step functions\n",
    "Functions that merge pairs of of triples to create merged triples. Concurrently creates joint probability matrices for all merged triples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_cherries(triples, cherries, learning_rate=0.01, steps=3000, info_decay=0.9, verbose=False):\n",
    "    keepers = []\n",
    "    to_be_merged = defaultdict(lambda: [None, None])\n",
    "    cherry_names, cherry_Ps = cherries\n",
    "    for names, F in triples:\n",
    "        num_uncommon = len(set(names) - set(cherry_names))\n",
    "        if num_uncommon == 3:\n",
    "            keepers.append((names, F))\n",
    "        elif num_uncommon == 2:\n",
    "            uncommon = frozenset(names) - frozenset(cherry_names)\n",
    "            name = (set(names) - uncommon).pop()\n",
    "            new_names = sorted(uncommon) + [name]\n",
    "            ix = [names.index(n) for n in new_names]\n",
    "            J = F.transpose(ix).astype(np.float32)\n",
    "            J /= J.sum()\n",
    "            N = F.sum()  # make no mistake, this is a kludge\n",
    "            to_be_merged[tuple(new_names[:2])][name == cherry_names[1]] = J\n",
    "\n",
    "    @tf.function()\n",
    "    def transform(params):\n",
    "        return tf.reshape(tfb.SoftmaxCentered()(params), (4, 4, 4))\n",
    "    \n",
    "    @tf.function()\n",
    "    def _loss(params_data):\n",
    "        params, data = params_data\n",
    "        J = transform(params)\n",
    "        twoJs = [J @ cherry_Ps[0], J @ cherry_Ps[1]]\n",
    "        # could do better managing the variance here\n",
    "        loss = tf.reduce_sum(tf.keras.losses.KLDivergence()(J, data))\n",
    "        return loss\n",
    "    \n",
    "    @tf.function()\n",
    "    def loss(params, data):\n",
    "        return tf.reduce_sum(tf.vectorized_map(_loss, (params, data)))\n",
    "\n",
    "    @tf.function()\n",
    "    def training_step(parameters, data, optimizer):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss_value = loss(parameters, data)\n",
    "        gradients = tape.gradient(loss_value, parameters)\n",
    "        return loss_value, gradients\n",
    "    \n",
    "    def mle_fit(data, loss, parameters, optimizer, steps, verbose):\n",
    "        for i in range(steps):\n",
    "            loss_value, gradients = training_step(parameters, data, optimizer)\n",
    "            optimizer.apply_gradients([(gradients, parameters)])\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                if verbose:\n",
    "                    iter_info = f\"Step: {optimizer.iterations.numpy()}, initial loss: {loss_value.numpy()}\"\n",
    "                    print(iter_info)\n",
    "    \n",
    "    normal_initializer = tf.random_normal_initializer()\n",
    "    K = len(to_be_merged)\n",
    "    parameters = tf.Variable(normal_initializer(shape=[K, 63], dtype=tf.float32), name='params')\n",
    "    data = tf.stack(list(to_be_merged.values()))\n",
    "    \n",
    "    optimizer = tf.optimizers.Adam(learning_rate=learning_rate)\n",
    "    mle_fit(data, loss, parameters, optimizer, steps=steps, verbose=verbose)\n",
    "    \n",
    "    for names, params in zip(to_be_merged, parameters.numpy()):\n",
    "        keepers.append((list(names) + ['-'.join(cherry_names)], info_decay*N*transform(params).numpy()))\n",
    "        \n",
    "    return keepers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd83a62a",
   "metadata": {},
   "source": [
    "## Tree building algorithm\n",
    "Functions for using cherry-picking to build trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13b5ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tree(forest, cherries):\n",
    "    new_node = PhyloNode('-'.join(cherries[0]))\n",
    "    for name, P in zip(*cherries):\n",
    "        child = forest[name]\n",
    "        child.P = P\n",
    "        new_node.append(child)\n",
    "        del forest[name]\n",
    "    forest[new_node.name] = new_node\n",
    "\n",
    "def triple_threat(triples, cherries_share_matrices=False,\n",
    "                  learning_rate=0.01, steps=3000, info_decay=0.9, verbose=False):\n",
    "    triples = list(triples)\n",
    "    forest = {n: PhyloNode(n) for names, _ in triples for n in names}\n",
    "    while True:\n",
    "        if verbose:\n",
    "            print('Looking for cherries')\n",
    "        losses, fits = fit_triples(triples, cherries_share_matrices=cherries_share_matrices,\n",
    "                                   learning_rate=learning_rate, steps=steps, verbose=verbose)\n",
    "        cherry_names = get_cherries(triples, losses, verbose=verbose)\n",
    "        if verbose:\n",
    "            print('Fitting cherries')\n",
    "        if not cherries_share_matrices:\n",
    "            triples_for_cherry = [t for t in triples if set(cherry_names) < set(t[0])]\n",
    "            _, fits = fit_triples(triples_for_cherry, cherries_share_matrices=True,\n",
    "                                  learning_rate=learning_rate, steps=steps, verbose=verbose)\n",
    "            cherry_Ps = get_Ps(cherry_names, triples_for_cherry, fits)\n",
    "        else:\n",
    "            cherry_Ps = get_Ps(cherry_names, triples, fits)\n",
    "        cherries = cherry_names, cherry_Ps\n",
    "        update_tree(forest, cherries)\n",
    "        if verbose:\n",
    "            for tree in forest.values():\n",
    "                print(tree)\n",
    "        if len(triples) == 1:\n",
    "            break\n",
    "        if verbose:\n",
    "            print('Merging cherries')\n",
    "        triples = merge_cherries(triples, cherries, learning_rate=learning_rate, steps=steps, \n",
    "                                 info_decay=info_decay, verbose=verbose)\n",
    "    tree = PhyloNode('-'.join(forest.keys()))\n",
    "    tree.pi = fits[0][0][0]\n",
    "    for name, child in forest.items():\n",
    "        if name in triples[0][0]:\n",
    "            fit = fits[0][triples[0][0].index(name)]\n",
    "            Pa = fit[1]\n",
    "            Pm = fit[2]\n",
    "    for name, child in forest.items():\n",
    "        child.P = Pa if name in triples[0][0] else Pm\n",
    "        tree.append(child)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b347257",
   "metadata": {},
   "source": [
    "# Some examples\n",
    "## Example 1\n",
    "Fit a rooted phylogeny of 5 mammals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05c519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aln = load_aligned_seqs('/home/ben/Data/pentads/ENSG00000197102.fa.gz', moltype=\"dna\")\n",
    "# aln = load_aligned_seqs('/home/ben/Data/pentads/ENSG00000131018.fa.gz', moltype=\"dna\")\n",
    "# aln = load_aligned_seqs('/home/ben/Data/pentads/ENSG00000179869.fa.gz', moltype=\"dna\")\n",
    "aln = load_aligned_seqs('brca1.fasta', moltype='dna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930a08fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "subaln = aln.get_similar(aln.take_seqs(['Human']).seqs[0],\n",
    "                      min_similarity=0.83)\n",
    "subaln"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097ad3f8",
   "metadata": {},
   "source": [
    "### All at once\n",
    "First run `triple-threat` all the way through. Run time is not fantastic for five taxa, but it also fits non-stationary models to all edges, and is expected to scale well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc09993",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "triples = get_triples(subaln, codon_position=3, verbose=True)\n",
    "tree = triple_threat(triples, cherries_share_matrices=False, info_decay=0.9, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c46792",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = tree.get_figure(\"radial\", width=600, height=600)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c248a02",
   "metadata": {},
   "source": [
    "## Example 2\n",
    "### A single iteration\n",
    "Now run through a single iteration of the algorithm.\n",
    "#### Fit triples\n",
    "This step fits rooted, strand-symmetric, continuous-time models to every taxon triple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b171e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "triples = get_triples(aln, verbose=True)\n",
    "losses, fits = fit_triples(triples, cherries_share_matrices=False, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d94ad1",
   "metadata": {},
   "source": [
    "### Find the most probably cherry\n",
    "This step finds the pair of taxa that are most likely to be a cherries using model selection calculations, then simultaneously fits that cherry across all of the triples of which it is a part to estimate its rate matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407c1108",
   "metadata": {},
   "outputs": [],
   "source": [
    "cherry_names = get_cherries(triples, losses, verbose=True)\n",
    "triples_for_cherry = [t for t in triples if set(cherry_names) < set(t[0])]\n",
    "_, fits = fit_triples(triples_for_cherry, cherries_share_matrices=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd238ae0",
   "metadata": {},
   "source": [
    "This step extracts the simultaneously fitted transition probability matrices for the cherry taxa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592eaaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cherry_Ps = get_Ps(cherry_names, triples_for_cherry, fits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b4d958",
   "metadata": {},
   "source": [
    "Test a single agglomeration step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ce671b",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = {}\n",
    "cherries = cherry_names, cherry_Ps\n",
    "update_tree(forest, cherries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fc9043",
   "metadata": {},
   "source": [
    "These were the cherry taxa and their corresponding transition probability matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2aadcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cherries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca9937",
   "metadata": {},
   "outputs": [],
   "source": [
    "forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9eadf0",
   "metadata": {},
   "source": [
    "These were the data that were fitted simultaneously to find the above rate matrics for Dog and Human."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510fee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_for_cherry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd4417b",
   "metadata": {},
   "source": [
    "### Merge the cherries\n",
    "Merge the cherries by fitting the triples between the unmerged taxa and the new merged node, using the probability matrices that we fitted above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a71b25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "triples = merge_cherries(triples, cherries, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62db9549",
   "metadata": {},
   "source": [
    "At the end of the first iteration, the triples now consist of unmerged taxa and the newly created merged node (called \"Dog-Human\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad536c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "triples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
