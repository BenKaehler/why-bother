{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd1779bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d23a4c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 12:52:14.001997: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c10fddee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-31 12:52:14.931864: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2021-08-31 12:52:14.956644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-31 12:52:14.956925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1716] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce RTX 2080 with Max-Q Design computeCapability: 7.5\n",
      "coreClock: 1.095GHz coreCount: 46 deviceMemorySize: 7.79GiB deviceMemoryBandwidth: 357.69GiB/s\n",
      "2021-08-31 12:52:14.956942: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "2021-08-31 12:52:14.958055: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcublas.so.10\n",
      "2021-08-31 12:52:14.959156: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcufft.so.10\n",
      "2021-08-31 12:52:14.959342: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcurand.so.10\n",
      "2021-08-31 12:52:14.960460: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusolver.so.10\n",
      "2021-08-31 12:52:14.961088: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcusparse.so.10\n",
      "2021-08-31 12:52:14.963606: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudnn.so.7\n",
      "2021-08-31 12:52:14.963708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-31 12:52:14.964027: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:982] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-08-31 12:52:14.964274: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1858] Adding visible gpu devices: 0\n"
     ]
    }
   ],
   "source": [
    "# thanks https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "  tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13923ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pa = np.array([[0.71893144, 0.14144765, 0.10698649, 0.03263443],\n",
    "       [0.06325843, 0.77423272, 0.13928361, 0.02322523],\n",
    "       [0.02322523, 0.13928361, 0.77423272, 0.06325843],\n",
    "       [0.03263443, 0.10698649, 0.14144765, 0.71893144]])\n",
    "\n",
    "Pb = np.array([[0.95154866, 0.01674797, 0.0234783 , 0.00822507],\n",
    "       [0.03863562, 0.84803839, 0.04732459, 0.0660014 ],\n",
    "       [0.0660014 , 0.04732459, 0.84803839, 0.03863562],\n",
    "       [0.00822507, 0.0234783 , 0.01674797, 0.95154866]])\n",
    "\n",
    "Pc = np.array([[0.88137945, 0.04613594, 0.02686734, 0.04561727],\n",
    "       [0.05009116, 0.9236776 , 0.0194023 , 0.00682894],\n",
    "       [0.00682894, 0.0194023 , 0.9236776 , 0.05009116],\n",
    "       [0.04561727, 0.02686734, 0.04613594, 0.88137945]])\n",
    "Pm = np.array([[0.81203158, 0.04399917, 0.07848442, 0.06548484],\n",
    "       [0.08284662, 0.87115553, 0.02024715, 0.0257507 ],\n",
    "       [0.0257507 , 0.02024715, 0.87115553, 0.08284662],\n",
    "       [0.06548484, 0.07848442, 0.04399917, 0.81203158]])\n",
    "\n",
    "pi = np.array([0.14977394, 0.26317362, 0.39165429, 0.19539816])\n",
    "\n",
    "pi_m = pi @ Pm\n",
    "\n",
    "J = np.array([[[0.07508613297288518, 0.00466350788803318, 0.0033432901131033796, 0.004042219322757277], [0.002186937387234338, 0.01570096614179908, 0.0011254388766303509, 0.0005015574893929194], [0.002007283482847851, 0.0012535337368050612, 0.013395860029779497, 0.0010220788989917808], [0.0013011086711092986, 0.0016042909684947522, 0.0012404869820179004, 0.011323417297654589]], [[0.03129698648252512, 0.00815953485072002, 0.004394876926278819, 0.0020308813605868174], [0.008288184321098279, 0.14201837079233715, 0.005398759767993272, 0.0017880581744532536], [0.0015277493099248596, 0.00886482820182737, 0.04269081732021271, 0.0028169181161841477], [0.0021015052010260205, 0.011819352252411985, 0.0034103593745217937, 0.02379160092199082]], [[0.02174460265532043, 0.002941403304710777, 0.017006110729978843, 0.0023681733984058486], [0.0022577498981608663, 0.03237071808690972, 0.012426756488990255, 0.0019160961759307586], [0.0022144999652547693, 0.006239396030284971, 0.20953281339937846, 0.012130269098422072], [0.002543170970124578, 0.003973434481032667, 0.011775618913078878, 0.04210892174683528]], [[0.012093125924573445, 0.0012993605630256348, 0.0021464967072447926, 0.0015645368675293004], [0.0010702747292501203, 0.013479197587703494, 0.0016493822206027895, 0.0025896804160072057], [0.0005894417935184289, 0.00127918145334602, 0.022260289874459077, 0.0029430040436876307], [0.005229012301832341, 0.0040480932067737535, 0.006152859243062461, 0.09785943409093556]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65573e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "J_mb = pi_m[:, np.newaxis] * Pb\n",
    "J_bc = J_mb.T @ Pc\n",
    "J_ab = Pa.T @ np.diag(pi) @ Pm @ Pb\n",
    "J_ca = (Pa.T @ np.diag(pi) @ Pm @ Pc).T\n",
    "\n",
    "assert np.allclose(J_bc, J.sum(axis=0))  # checking I have constructed it correctly\n",
    "assert np.allclose(J_ab, J.sum(axis=2))\n",
    "assert np.allclose(J_ca, J.sum(axis=1).T)\n",
    "\n",
    "J_einsum = np.einsum('i,ij,ik,ku,kv', pi, Pa, Pm, Pb, Pc)\n",
    "assert np.allclose(J_einsum, J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d7b99c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(N, J):\n",
    "    return sps.multinomial.rvs(N, J.flatten()).reshape((4, 4, 4))/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "079f283d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thanks https://github.com/mlgxmez/thelongrun_notebooks/blob/master/MLE_tutorial.ipynb\n",
    "\n",
    "@tf.function()\n",
    "def transform_P_matrix(params):\n",
    "    params = tf.exp(params)\n",
    "    Q0 = tf.concat([[-tf.reduce_sum(params[0])], params[0]],\n",
    "                   axis=0)\n",
    "    Q1 = tf.concat([[params[1,0]], [-tf.reduce_sum(params[1])], params[1,1:]],\n",
    "                   axis=0)\n",
    "    Q = tf.concat([[Q0], [Q1], [Q1[::-1]], [Q0[::-1]]], axis=0)\n",
    "    return tf.linalg.expm(Q)\n",
    "\n",
    "@tf.function()\n",
    "def transform(params):\n",
    "    pi = tfb.SoftmaxCentered()(params[0])\n",
    "    Pa = transform_P_matrix(params[1:3])\n",
    "    Pm = transform_P_matrix(params[3:5])\n",
    "    Pb = transform_P_matrix(params[5:7])\n",
    "    Pc = transform_P_matrix(params[7:9])\n",
    "    return pi, Pa, Pm, Pb, Pc\n",
    "    \n",
    "@tf.function()\n",
    "def _loss(params_data):\n",
    "    params, data = params_data\n",
    "    pi, Pa, Pm, Pb, Pc = transform(params)\n",
    "    J = tf.einsum('i,ij,ik,ku,kv', pi, Pa, Pm, Pb, Pc)\n",
    "    loss = tf.reduce_sum(tf.keras.losses.KLDivergence()(J, data))\n",
    "    return loss\n",
    "\n",
    "@tf.function()\n",
    "def loss(params, data):\n",
    "    return tf.reduce_sum(tf.vectorized_map(_loss, (parameters, data)))\n",
    "\n",
    "@tf.function()\n",
    "def training_step(parameters, data, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(parameters, data)\n",
    "    gradients = tape.gradient(loss_value, parameters)\n",
    "    return loss_value, gradients\n",
    "\n",
    "def mle_run(data, loss, parameters, optimizer, steps=500, verbose=False):\n",
    "    \n",
    "    for i in range(steps):\n",
    "        loss_value, gradients = training_step(parameters, data, optimizer)\n",
    "        optimizer.apply_gradients([(gradients, parameters)])\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            iter_info = f\"Step: {optimizer.iterations.numpy()}, initial loss: {loss_value.numpy()}\"\n",
    "            if verbose:\n",
    "                print(iter_info)\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ffc4eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using a while_loop for converting ResourceGather\n",
      "Step: 1, initial loss: 0.20222054421901703\n",
      "Step: 101, initial loss: 0.16597172617912292\n",
      "Step: 201, initial loss: 0.09657593071460724\n",
      "Step: 301, initial loss: 0.07258816063404083\n",
      "Step: 401, initial loss: 0.05790988355875015\n",
      "Step: 501, initial loss: 0.050487734377384186\n",
      "Step: 601, initial loss: 0.030205361545085907\n",
      "Step: 701, initial loss: 0.006704481318593025\n",
      "Step: 801, initial loss: 0.0014757821336388588\n",
      "Step: 901, initial loss: 0.0007248119218274951\n",
      "Step: 1001, initial loss: 0.0006472094682976604\n",
      "Step: 1101, initial loss: 0.000616406206972897\n",
      "Step: 1201, initial loss: 0.0005991252255626023\n",
      "Step: 1301, initial loss: 0.0005879321834072471\n",
      "Step: 1401, initial loss: 0.0005801510415039957\n",
      "Step: 1501, initial loss: 0.000574370613321662\n",
      "Step: 1601, initial loss: 0.0005698658060282469\n",
      "Step: 1701, initial loss: 0.000566203729249537\n",
      "Step: 1801, initial loss: 0.0005631373496726155\n",
      "Step: 1901, initial loss: 0.0005604737671092153\n",
      "Step: 2001, initial loss: 0.0005581151926890016\n",
      "Step: 2101, initial loss: 0.000556000042706728\n",
      "Step: 2201, initial loss: 0.0005541174905374646\n",
      "Step: 2301, initial loss: 0.0005524201551452279\n",
      "Step: 2401, initial loss: 0.0005509020411409438\n",
      "Step: 2501, initial loss: 0.0005495280493050814\n",
      "Step: 2601, initial loss: 0.0005482997512444854\n",
      "Step: 2701, initial loss: 0.0005471787881106138\n",
      "Step: 2801, initial loss: 0.000546159571968019\n",
      "Step: 2901, initial loss: 0.0005452240002341568\n",
      "Step: 3001, initial loss: 0.0005443625850602984\n",
      "Step: 3101, initial loss: 0.000543588656000793\n",
      "Step: 3201, initial loss: 0.0005428560543805361\n",
      "Step: 3301, initial loss: 0.0005421823589131236\n",
      "Step: 3401, initial loss: 0.0005415617488324642\n",
      "Step: 3501, initial loss: 0.0005409792065620422\n",
      "Step: 3601, initial loss: 0.0005404392722994089\n",
      "Step: 3701, initial loss: 0.0005398790235631168\n",
      "Step: 3801, initial loss: 0.000539375701919198\n",
      "Step: 3901, initial loss: 0.0005388918216340244\n",
      "Step: 4001, initial loss: 0.0005384359392337501\n",
      "Step: 4101, initial loss: 0.0005379992653615773\n",
      "Step: 4201, initial loss: 0.0005375605542212725\n",
      "Step: 4301, initial loss: 0.0005371343577280641\n",
      "Step: 4401, initial loss: 0.0005367186386138201\n",
      "Step: 4501, initial loss: 0.0005363108357414603\n",
      "Step: 4601, initial loss: 0.0005359422066248953\n",
      "Step: 4701, initial loss: 0.0005355555331334472\n",
      "Step: 4801, initial loss: 0.0005351930158212781\n",
      "Step: 4901, initial loss: 0.0005348393460735679\n",
      "Step: 5001, initial loss: 0.000534502265509218\n",
      "Step: 5101, initial loss: 0.0005341927753761411\n",
      "Step: 5201, initial loss: 0.0005338644841685891\n",
      "Step: 5301, initial loss: 0.0005335576133802533\n",
      "Step: 5401, initial loss: 0.0005332626169547439\n",
      "Step: 5501, initial loss: 0.0005329896230250597\n",
      "Step: 5601, initial loss: 0.0005327173275873065\n",
      "Step: 5701, initial loss: 0.0005324537632986903\n",
      "Step: 5801, initial loss: 0.000532223901245743\n",
      "Step: 5901, initial loss: 0.0005319812917150557\n",
      "Step: 6001, initial loss: 0.0005317621980793774\n",
      "Step: 6101, initial loss: 0.0005315504386089742\n",
      "Step: 6201, initial loss: 0.0005313322762958705\n",
      "Step: 6301, initial loss: 0.0005311398999765515\n",
      "Step: 6401, initial loss: 0.0005309791304171085\n",
      "Step: 6501, initial loss: 0.0005307939136400819\n",
      "Step: 6601, initial loss: 0.0005306069506332278\n",
      "Step: 6701, initial loss: 0.0005304731894284487\n",
      "Step: 6801, initial loss: 0.0005303050857037306\n",
      "Step: 6901, initial loss: 0.0005301791825331748\n",
      "Step: 7001, initial loss: 0.0005300504853948951\n",
      "Step: 7101, initial loss: 0.0005299280164763331\n",
      "Step: 7201, initial loss: 0.0005297894822433591\n",
      "Step: 7301, initial loss: 0.0005296866875141859\n",
      "Step: 7401, initial loss: 0.0005295780138112605\n",
      "Step: 7501, initial loss: 0.0005294576985761523\n",
      "Step: 7601, initial loss: 0.0005293750436976552\n",
      "Step: 7701, initial loss: 0.000529276323504746\n",
      "Step: 7801, initial loss: 0.0005291637498885393\n",
      "Step: 7901, initial loss: 0.000529102107975632\n",
      "Step: 8001, initial loss: 0.0005290047847665846\n",
      "Step: 8101, initial loss: 0.0005289281252771616\n",
      "Step: 8201, initial loss: 0.0005288428510539234\n",
      "Step: 8301, initial loss: 0.0005287746898829937\n",
      "Step: 8401, initial loss: 0.0005287285312078893\n",
      "Step: 8501, initial loss: 0.0005286629311740398\n",
      "Step: 8601, initial loss: 0.0005285980296321213\n",
      "Step: 8701, initial loss: 0.0005285567021928728\n",
      "Step: 8801, initial loss: 0.000528496690094471\n",
      "Step: 8901, initial loss: 0.0005284344078972936\n",
      "Step: 9001, initial loss: 0.0005283922655507922\n",
      "Step: 9101, initial loss: 0.0005283313803374767\n",
      "Step: 9201, initial loss: 0.0005283160717226565\n",
      "Step: 9301, initial loss: 0.0005282561760395765\n",
      "Step: 9401, initial loss: 0.0005282201454974711\n",
      "Step: 9501, initial loss: 0.0005281968042254448\n",
      "Step: 9601, initial loss: 0.0005281385965645313\n",
      "Step: 9701, initial loss: 0.0005281193880364299\n",
      "Step: 9801, initial loss: 0.0005280862678773701\n",
      "Step: 9901, initial loss: 0.000528041273355484\n",
      "CPU times: user 3min 55s, sys: 36.3 s, total: 4min 32s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "N = 10000\n",
    "K = 3\n",
    "normal_initializer = tf.random_normal_initializer()\n",
    "parameters = tf.Variable(normal_initializer(shape=[K, 9, 3], dtype=tf.float32), name='params')\n",
    "Ja = sample(N, J).astype(np.float32)\n",
    "Jb = Ja.transpose([1, 2, 0])\n",
    "Jc = Ja.transpose([2, 0, 1])\n",
    "data = tf.stack([Ja, Jb, Jc])\n",
    "\n",
    "optimizer = tf.optimizers.Adam(0.01)\n",
    "update_list = mle_run(data, loss, parameters, optimizer, steps=10000, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2dcc4fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14977394 0.26317362 0.39165429 0.19539816]\n",
      "[0.12204078 0.28609538 0.433098   0.15876588]\n",
      "\n",
      "[[0.71893144 0.14144765 0.10698649 0.03263443]\n",
      " [0.06325843 0.77423272 0.13928361 0.02322523]\n",
      " [0.02322523 0.13928361 0.77423272 0.06325843]\n",
      " [0.03263443 0.10698649 0.14144765 0.71893144]]\n",
      "[[0.8098485  0.0768582  0.06198487 0.0513085 ]\n",
      " [0.07545466 0.7581055  0.13507761 0.03136224]\n",
      " [0.03136223 0.13507761 0.75810546 0.07545465]\n",
      " [0.0513085  0.06198487 0.07685821 0.80984855]]\n",
      "\n",
      "[[0.81203158 0.04399917 0.07848442 0.06548484]\n",
      " [0.08284662 0.87115553 0.02024715 0.0257507 ]\n",
      " [0.0257507  0.02024715 0.87115553 0.08284662]\n",
      " [0.06548484 0.07848442 0.04399917 0.81203158]]\n",
      "[[0.8716503  0.00613635 0.0736386  0.04857485]\n",
      " [0.11426511 0.81730264 0.02106063 0.0473716 ]\n",
      " [0.0473716  0.02106064 0.81730264 0.11426511]\n",
      " [0.04857485 0.0736386  0.00613635 0.8716503 ]]\n",
      "\n",
      "[[0.95154866 0.01674797 0.0234783  0.00822507]\n",
      " [0.03863562 0.84803839 0.04732459 0.0660014 ]\n",
      " [0.0660014  0.04732459 0.84803839 0.03863562]\n",
      " [0.00822507 0.0234783  0.01674797 0.95154866]]\n",
      "[[0.9450681  0.0237685  0.02302863 0.00813475]\n",
      " [0.04002118 0.8475731  0.04366479 0.06874093]\n",
      " [0.06874094 0.04366479 0.8475731  0.04002118]\n",
      " [0.00813475 0.02302863 0.0237685  0.9450681 ]]\n",
      "\n",
      "[[0.88137945 0.04613594 0.02686734 0.04561727]\n",
      " [0.05009116 0.9236776  0.0194023  0.00682894]\n",
      " [0.00682894 0.0194023  0.9236776  0.05009116]\n",
      " [0.04561727 0.02686734 0.04613594 0.88137945]]\n",
      "[[0.8827951  0.0427367  0.0268471  0.04762108]\n",
      " [0.04423704 0.92613524 0.0209828  0.00864486]\n",
      " [0.00864486 0.0209828  0.9261353  0.04423704]\n",
      " [0.04762108 0.0268471  0.0427367  0.8827951 ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for p, phat in zip([pi, Pa, Pm, Pb, Pc], transform(parameters[0])):\n",
    "    print(p)\n",
    "    print(phat.numpy())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d04a6ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00012748 0.00020096 0.00019958]\n",
      "[0.5086934  0.24395621 0.24735038]\n"
     ]
    }
   ],
   "source": [
    "ls =  np.array([_loss((parameters[i], data[i])).numpy() for i in range(K)])\n",
    "print(ls)\n",
    "ls = N*ls\n",
    "delta = ls - ls.min()\n",
    "weights = np.exp(-delta)\n",
    "weights /= weights.sum()\n",
    "# ls /= ls.sum()\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb2d939",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
